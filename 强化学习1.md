# Reinforcement-Learning1

![img](https://datawhalechina.github.io/leedeeprl-notes/chapter1/img/1.1.png)

## Basic concept
How can an agent maximize the rewards in a complex and uncertain environment? 
The schematic diagram consists of two parts: agent and environment. During the reinforcement learning process, the agent and the environment are always interacting. 
The agent obtains the state in the environment, and the agent uses this state an action and a decision as output. Then this decision will be put into the environment, 
and the environment will use the decision that agent takes to output the next state and the reward for the current decision. 
The purpose of the agent is to get as many rewards from the environment as possible.

强化学习讨论的问题是一个 智能体(agent) 怎么在一个复杂不确定的环境(environment)里面去极大化它能获得的奖励。示意图由两部分组成：agent 和environment。在强化学习过程中，agent 跟 environment 一直在交互。Agent 在环境里面获取到状态，agent 会利用这个状态输出一个 action，一个决策。然后这个决策会放到环境之中去，环境会通过这个 agent 采取的决策，输出下一个状态以及当前的这个决策得到的奖励。Agent 的目的就是为了尽可能多地从环境中获取奖励。

## Sequential Decision Making

### Reward
- A reward is a scalar feedback signal
- Indicate how well agent is doing at step t
- Reinforcement Learning is based on the maximization of rewards:
All goals of the agent can be described by the maximization of expected
cumulative reward.

### What is Sequential Decision Making?
- The history is the sequence of observations, actions, rewards.
- What happens next depends on the history
- State is the function used to determine what happens next
                      St = f(Ht)

- Environment state and agent state
- Full observability: agent directly observes the environment state,
formally as Markov decision process (MDP)
- Partial observability: agent indirectly observes the environment,
formally as partially observable Markov decision process
(POMDP) -> Black jack (only see public cards), Atari game with pixel observation.

强化学习的训练数据就是这样一个玩游戏的过程。你从第一步开始，采取一个决策，比如说你把这个往右移，接到这个球了。第二步你又做出决策，得到的 trainingdata 是一个玩游戏的序列。

比如现在是在第三步，你把这个序列放进去，你希望这个网络可以输出一个决策，在当前的这个状态应该输出往右移或者往左移。这里有个问题，就是我们没有标签来说明你现在这个动作是正确还是错误，必须等到这个游戏结束可能，这个游戏可能十秒过后才结束。现在这个动作往左往右到底是不是对最后游戏的结束能赢有帮助，其实是不清楚的的。这里就面临一个 延迟奖励(Delayed Reward) ，所以就使得训练这个网络非常困难。

## Major Components of an RL Agent

An RL agent may include one or more of these
components:
- Policy: agent’s behavior function
- Value function: how good is each state or action
- Model: agent’s state representation of the environment

### Policy
![img](https://datawhalechina.github.io/leedeeprl-notes/chapter1/img/1.26.png)

![img](https://datawhalechina.github.io/leedeeprl-notes/chapter1/img/1.31.png)

我们深入看这三个组成成分的一些细节。

Policy 决定了这个 agent 的行为，它其实是一个函数，把输入的状态变成行为。这里有两种 policy：

1. 一种是 stochastic policy(随机性策略) ，它就是 函数 。当你输入一个状态 的时候，输出是一个概率。这个概率就是你所有行为的一个概率，然后你可以进一步对这个概率分布进行采样，得到真实的你采取的行为。比如说这个概率可能是有 70% 的概率往左，30% 的概率往右，那么你通过采样就可以得到一个 action。

2. 一种是 deterministic policy(确定性策略) ，就是说你这里有可能只是采取它的极大化，采取最有可能的动作。你现在这个概率就是事先决定好的。

从 Atari 游戏来看的话，policy function 的输入就是游戏的一帧，它的输出决定你是往左走或者是往右走。

通常情况下，强化学习一般使用 随机性策略 。随机性策略有很多优点：

- 在学习时可以通过引入一定随机性来更好地探索环境；
- 随机性策略的动作具有多样性，这一点在多个智能体博弈时也非常重要。采用确定性策略的智能体总是对同样的环境做出相同的动作，会导致它的策略很容易被对手预测。

### Value Function
Value function: expected discounted sum of future rewards under a
particular policy
- Discount factor weights immediate vs future rewards
- Used to quantify goodness/badness of states and actions
- Q-function (could be used to select among actions)

![img](https://datawhalechina.github.io/leedeeprl-notes/chapter1/img/1.32.png)

### Model

![img](https://datawhalechina.github.io/leedeeprl-notes/chapter1/img/1.29.png)

### Types of RL Agents based on What the Agent Learns
- Value-based agent:
      - Explicit: Value function
      - Implicit: Policy (can derive a policy from value function)
- Policy-based agent:
      - Explicit: policy
      - No value function
- Actor-Critic agent:
      - Explicit: policy and value function


### Types of RL Agents on if there is model
- Model-based
      - Explicit: model
      - May or may not have policy and/or value function
- Model-free
      - Explicit: value function and/or policy function
      - No model.

![img](https://datawhalechina.github.io/leedeeprl-notes/chapter1/img/1.36.png)

### Exploration and Exploitation
- Agent only experiences what happens for the actions it tries!
- How should an RL agent balance its actions?
       - Exploration: trying new things that might enable the agent to make better
          decisions in the future
       - Exploitation: choosing actions that are expected to yield good reward given
          the past experience
- Often there may be an exploration-exploitation trade-off
       - May have to sacrifice reward in order to explore & learn about potentially
          better policy


```python

```
